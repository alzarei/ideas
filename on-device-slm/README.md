# On-Device LLM Assistant

A complete solution for running small language models locally with an intuitive web interface. Features conversation management, model switching, and intelligent system analysis capabilities.

## âš¡ Quick Start

### Option 1: Automated Setup (Recommended)
```bash
# Clone the repository
git clone <repository-url>
cd on-device-slm

# Run the automated setup
python setup.py
```

### Option 2: Manual Setup
```bash
# 1. Install system dependencies
# - Python 3.8+ 
# - Node.js 16+
# - Ollama (https://ollama.ai)

# 2. Run setup commands
python setup.py         # Full automated setup
# OR
.\dev.ps1 setup         # Windows PowerShell
# OR  
make setup              # If you have Make installed
```

### Option 3: Development Scripts
**Windows:**
```powershell
.\dev.ps1 start         # Start application
.\dev.ps1 start-dev     # Development mode
.\dev.ps1 help          # See all commands
```

**Unix/Linux/macOS:**
```bash
make start              # Start application  
make start-dev          # Development mode
make help               # See all commands
```

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ backend/              # FastAPI backend server
â”‚   â”œâ”€â”€ main.py          # Main API server
â”‚   â”œâ”€â”€ conversation_manager.py  # Chat state management
â”‚   â””â”€â”€ fallback.html    # Fallback UI
â”œâ”€â”€ frontend/            # React TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # React components
â”‚   â”‚   â””â”€â”€ hooks/       # Custom hooks
â”‚   â””â”€â”€ build/           # Production build
â”œâ”€â”€ config/              # Configuration files
â”‚   â”œâ”€â”€ models.json      # Available LLM models
â”‚   â”œâ”€â”€ model_manager.py # Model management
â”‚   â””â”€â”€ training_config.py
â”œâ”€â”€ examples/            # Usage examples
â”œâ”€â”€ writing_samples/     # Training samples
â”œâ”€â”€ setup.py            # Automated setup script
â”œâ”€â”€ launcher.py         # Application launcher
â”œâ”€â”€ dev.ps1             # PowerShell dev script
â””â”€â”€ Makefile            # Make development tasks
```

## ğŸš€ Features

### Core Functionality
- **Conversation Management**: Persistent chat history with context switching
- **Multiple Models**: Support for various Ollama models with easy switching
- **Web Interface**: Modern React frontend with real-time updates
- **Local Processing**: Complete privacy with on-device inference
- **Token Management**: Smart context window handling and conversation trimming

### Models Included
- **Llama 3.2 3B/1B** - Balanced performance and efficiency
- **Dolphin Llama3 8B** - Enhanced reasoning capabilities  
- **Code Llama** - Specialized for code analysis
- **Gemma 2B** - Lightweight option
- **Phi-3 Mini** - Microsoft's efficient model
- **Wizard Vicuna** - Uncensored creative model

### Development Features
- **Hot Reload**: Automatic restart on code changes
- **API Documentation**: Built-in Swagger/OpenAPI docs
- **Health Monitoring**: System status endpoints
- **Modular Architecture**: Easy to extend and customize

## ğŸ“š Usage

### Starting the Application
```bash
# Quick start (automated)
python launcher.py

# Development mode (hot reload)
.\dev.ps1 start-dev      # Windows
make start-dev           # Unix/Linux/macOS
```

### Accessing the Interface
- **Main Application**: http://localhost:8000
- **API Documentation**: http://localhost:8000/api/docs  
- **Health Check**: http://localhost:8000/api/health
- **Frontend Only**: http://localhost:3000 (dev mode)

### Managing Models
```bash
# List available models
ollama list

# Pull new models
ollama pull llama3.2:3b
ollama pull dolphin-llama3:8b

# Configure models in config/models.json
```

## ğŸ› ï¸ Development

### Prerequisites
- **Python 3.8+** with pip
- **Node.js 16+** with npm
- **Ollama** ([Download](https://ollama.ai))
- **Git** for version control

### Development Workflow
```bash
# Full development setup
python setup.py

# Start backend only
.\dev.ps1 start-backend  # Windows
make start-backend       # Unix/Linux/macOS

# Start frontend only  
.\dev.ps1 start-frontend # Windows
make start-frontend      # Unix/Linux/macOS

# Run tests
.\dev.ps1 test          # Windows
make test               # Unix/Linux/macOS

# Clean build artifacts
.\dev.ps1 clean         # Windows
make clean              # Unix/Linux/macOS
```

### Adding New Models
1. Edit `config/models.json`
2. Add model configuration:
```json
{
  "id": "new-model:7b",
  "name": "New Model 7B", 
  "description": "Description of capabilities",
  "category": "general",
  "size_gb": 4.1,
  "context_window": 4096,
  "install_command": "ollama pull new-model:7b",
  "enabled": true
}
```
3. Pull the model: `ollama pull new-model:7b`
4. Restart the application

## ğŸ“‹ Available Commands

### PowerShell (Windows)
```powershell
.\dev.ps1 help          # Show all commands
.\dev.ps1 setup         # Complete setup
.\dev.ps1 start         # Start application
.\dev.ps1 start-dev     # Development mode
.\dev.ps1 build         # Build for production
.\dev.ps1 test          # Run tests
.\dev.ps1 clean         # Clean artifacts
.\dev.ps1 reset         # Reset environment
.\dev.ps1 models-list   # List Ollama models
.\dev.ps1 models-pull   # Download recommended models
```

### Make (Unix/Linux/macOS)
```bash
make help               # Show all commands
make setup              # Complete setup  
make start              # Start application
make start-dev          # Development mode
make build              # Build for production
make test               # Run tests
make clean              # Clean artifacts
make reset              # Reset environment
make models-list        # List Ollama models
make models-pull        # Download recommended models
```

## ğŸ”§ Configuration

### Environment Setup
The setup process automatically:
- Creates Python virtual environment
- Installs all dependencies
- Downloads recommended models
- Builds the frontend
- Creates startup scripts
- Generates development guide

### Manual Configuration
- **Models**: Edit `config/models.json`
- **Training**: Modify `config/training_config.py`
- **Backend**: Configure in `backend/main.py`
- **Frontend**: Customize in `frontend/src/`

## ğŸš€ Production Deployment

```bash
# Build for production
.\dev.ps1 build         # Windows
make build              # Unix/Linux/macOS

# The frontend will be built to frontend/build/
# Backend serves static files automatically
```

## ğŸ› Troubleshooting

### Common Issues

**Ollama not found:**
```bash
# Install Ollama
# Windows: Download from https://ollama.ai
# macOS: brew install ollama  
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve
```

**Python virtual environment issues:**
```bash
.\dev.ps1 reset         # Reset everything
.\dev.ps1 setup         # Rebuild environment
```

**Frontend build errors:**
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install
npm run build
```

**Port conflicts:**
- Backend: Change port in `launcher.py` (default: 8000)
- Frontend dev: Change in `frontend/package.json` (default: 3000)

### Getting Help
1. Check `DEVELOPMENT.md` for detailed guides
2. View API docs at http://localhost:8000/api/docs
3. Examine logs in the terminal output
4. Verify Ollama status: `ollama list`

## ğŸ“– Additional Documentation

- **DEVELOPMENT.md** - Detailed development guide
- **ARCHITECTURE.md** - System architecture overview
- **STYLE_TRAINING_GUIDE.md** - Model training guidelines
- **TOKEN_LIMITS_GUIDE.md** - Context management strategies

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/new-feature`
3. Make changes and test: `.\dev.ps1 test`
4. Commit changes: `git commit -am 'Add new feature'`
5. Push to branch: `git push origin feature/new-feature`
6. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.
